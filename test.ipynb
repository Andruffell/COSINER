{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anacl\\Documents\\GitHub\\COSINER\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using the latest cached version of the module from C:\\Users\\Anacl\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--seqeval\\541ae017dc683f85116597d48f621abc7b21b88dc42ec937c71af5415f0af63c (last modified on Tue Apr 18 00:53:25 2023) since it couldn't be found locally at evaluate-metric--seqeval, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "import utils\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import AutoModelForTokenClassification, Trainer, AutoTokenizer, DataCollatorForTokenClassification, TrainingArguments, AutoConfig\n",
    "\n",
    "dataset = load_from_disk(\"./data/ncbi.hf\")\n",
    "features = dataset[\"train\"].features\n",
    "label_list = features['ner_tags'].feature.names\n",
    "id2label = {i: label_list[i] for i in range(len(label_list))}\n",
    "label2id = {label_list[i]: i for i in range(len(label_list))}\n",
    "\n",
    "b_to_i_label = []\n",
    "for idx, label in enumerate(label_list):\n",
    "    if label.startswith(\"B-\") and label.replace(\"B-\", \"I-\") in label_list:\n",
    "        b_to_i_label.append(label_list.index(label.replace(\"B-\", \"I-\")))\n",
    "    else:\n",
    "        b_to_i_label.append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ./models/BERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on train dataset: 100%|██████████| 5425/5425 [00:01<00:00, 3708.08 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5425\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 500/3395 [01:54<10:06,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1006, 'learning_rate': 4.263622974963181e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 1001/3395 [03:40<08:18,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.041, 'learning_rate': 3.527245949926363e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 1500/3395 [05:29<06:43,  4.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0241, 'learning_rate': 2.7908689248895437e-05, 'epoch': 2.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 2000/3395 [07:17<04:57,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0126, 'learning_rate': 2.0544918998527246e-05, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 2500/3395 [09:02<03:04,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0065, 'learning_rate': 1.3181148748159059e-05, 'epoch': 3.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 3000/3395 [10:48<01:23,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0035, 'learning_rate': 5.817378497790869e-06, 'epoch': 4.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3395/3395 [12:13<00:00,  4.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 733.1539, 'train_samples_per_second': 36.998, 'train_steps_per_second': 4.631, 'train_loss': 0.027909896563360022, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "                                                        './models/BERT',\n",
    "                                                        cache_dir=None,\n",
    "                                                        num_labels=len(label_list), \n",
    "                                                        id2label=id2label, \n",
    "                                                        label2id=label2id,\n",
    "                                                        token=None,\n",
    "                                                        )\n",
    "\n",
    "trainer_args = TrainingArguments(\n",
    "                                optim=\"adamw_torch\",\n",
    "                                num_train_epochs=5,\n",
    "                                output_dir = str(\"./output\"),\n",
    "                                evaluation_strategy=\"no\",\n",
    "                                save_strategy=\"no\",\n",
    "                                do_eval=False,\n",
    "                                seed=100,\n",
    "                                full_determinism=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./models/BERT\", local_files_only=True, padding=True, num_labels=3)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "train_dataset = dataset['train'].map(\n",
    "                        utils.tokenize_and_align_labels,\n",
    "                        batched=True,\n",
    "                        desc=\"Running tokenizer on train dataset\", \n",
    "                        fn_kwargs={\"tokenizer\": tokenizer, \"b_to_i_label\": b_to_i_label}\n",
    "        ).remove_columns(['ner_tags', 'id', 'tokens'])\n",
    "\n",
    "print(train_dataset)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "            model=model.to(\"cuda:0\"),\n",
    "            train_dataset=train_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=utils.compute_metrics_wrapper(label_list, metric),\n",
    "            args = trainer_args,\n",
    "    )\n",
    "\n",
    "t = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = []\n",
    "metrics = t.metrics\n",
    "train_metrics.append(t.metrics)\n",
    "\n",
    "max_train_samples = len(train_dataset)\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_runtime': 733.1539,\n",
       " 'train_samples_per_second': 36.998,\n",
       " 'train_steps_per_second': 4.631,\n",
       " 'train_loss': 0.027909896563360022,\n",
       " 'epoch': 5.0,\n",
       " 'train_samples': 5425}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COSINER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
